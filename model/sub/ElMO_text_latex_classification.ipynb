{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6leHmF8x0fV"
   },
   "source": [
    "## 사전설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/kr-img2latex\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ubuntu/kr-img2latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: imbalanced-learn in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from imbalanced-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from imbalanced-learn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from imbalanced-learn) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from imbalanced-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: tensorflow-hub in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (0.14.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from tensorflow-hub) (1.24.3)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from tensorflow-hub) (4.23.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "id": "FVDGgqysxcVA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (4.31.0)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: kiwipiepy in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: dataclasses in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from kiwipiepy) (0.6)\n",
      "Requirement already satisfied: kiwipiepy-model~=0.15 in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from kiwipiepy) (0.15.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from kiwipiepy) (1.24.3)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.conda/envs/team_conda/lib/python3.10/site-packages (from kiwipiepy) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "!pip install --upgrade transformers\n",
    "from transformers import BertTokenizer, DonutProcessor\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob, shutil\n",
    "import xml.etree.ElementTree as elemTree\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Union, Tuple, Any\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "!pip install kiwipiepy\n",
    "from kiwipiepy import Kiwi, Match\n",
    "kiwi = Kiwi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "R7rsV6X20VxY"
   },
   "outputs": [],
   "source": [
    "LATEX_START = \"<latex>\"\n",
    "LATEX_END = \"</latex>\"\n",
    "\n",
    "new_tokens = [\n",
    "    LATEX_START,\n",
    "    LATEX_END\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQdOR30Ux3pR"
   },
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "XrLgc39Lx4zZ"
   },
   "outputs": [],
   "source": [
    "train_json_files = glob.glob('./data/train/annotations/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "u6IsAsFsHiwy"
   },
   "outputs": [],
   "source": [
    "test_json_files = glob.glob('./data/test/annotations/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7695,
     "status": "ok",
     "timestamp": 1689950493458,
     "user": {
      "displayName": "전현욱",
      "userId": "16513256288302189921"
     },
     "user_tz": -540
    },
    "id": "gktQJ5jBpTRo",
    "outputId": "e098646b-c54e-4ed9-8b85-3ef8d0e94c36"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150450/150450 [00:06<00:00, 24900.30it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_gt_string(files):\n",
    "    ground_truths = []\n",
    "    labels = []\n",
    "\n",
    "    for f in tqdm(files):\n",
    "        filename = os.path.basename(f)\n",
    "        filepath = Path(f)\n",
    "        \n",
    "        \n",
    "        with open(filepath,encoding='utf-8') as fp:\n",
    "            data = json.load(fp)\n",
    "            \n",
    "        label = filename.split('_')[2]\n",
    "        data_series = data['segments']\n",
    "\n",
    "        all_lines = []\n",
    "\n",
    "        NON_OBJECT = False\n",
    "        \n",
    "\n",
    "        for d_line in data_series:\n",
    "            if 'equation' not in d_line:\n",
    "                NON_OBJECT = True\n",
    "                break\n",
    "\n",
    "            if '$' not in d_line['equation']:\n",
    "                new_line = d_line['equation']\n",
    "            else:\n",
    "                equation = d_line['equation'].split('$')\n",
    "                latex_line = equation[0]\n",
    "                for i, e in enumerate(equation[1:]):\n",
    "                    if i%2 == 0:\n",
    "                        latex_line += LATEX_START + e\n",
    "                    else:\n",
    "                        latex_line += LATEX_END + e\n",
    "\n",
    "                new_line = latex_line\n",
    "        \n",
    "        all_lines.append(new_line)\n",
    "\n",
    "        if not NON_OBJECT:\n",
    "            ground_truths += all_lines\n",
    "            labels += [label] * len(all_lines)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return ground_truths, labels\n",
    "\n",
    "ground_truths, gt_labels = get_gt_string(train_json_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1689950493459,
     "user": {
      "displayName": "전현욱",
      "userId": "16513256288302189921"
     },
     "user_tz": -540
    },
    "id": "e90yI17pqn42",
    "outputId": "885da9e3-0634-4a92-d57d-446a78763738",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<latex>36+24+20+16+4=100 \\\\%</latex>', '<latex>y=-x+3+m</latex>.', '<latex>\\\\displaystyle 5</latex> <latex>\\\\displaystyle \\\\frac{1}{2},\\\\frac{1}{5}</latex>', '<latex>\\\\frac{100}{3600}+20</latex>', '5', '<latex>-1<2 \\\\cos x \\\\leq 1 \\\\quad,-\\\\frac{1}{2}<\\\\cos x \\\\leq \\\\frac{1}{2}</latex>', '<latex>68=1X68</latex>', '쪽 쪽', '<latex>-3\\\\cfrac{1}{3}</latex>을 수직선에 그려보면', '파 <latex>\\\\displaystyle \\\\sigma (X)=\\\\sqrt{V(X)}= \\\\;  \\\\sqrt{11}</latex>']\n",
      "['e6', 'm2', 'e4', 'm1', 'h1', 'h3', 'm1', 'e4', 'm1', 'h3']\n"
     ]
    }
   ],
   "source": [
    "print(ground_truths[:10])\n",
    "print(gt_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127060"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127060"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gt_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "1YJSBiJqQUvA"
   },
   "outputs": [],
   "source": [
    "labels = [\n",
    "    'e4','e5','e6',\n",
    "    'm1','m2','m3',\n",
    "    'h1','h2','h3'\n",
    "]\n",
    "\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "\n",
    "for i, item in enumerate(labels):\n",
    "    id2label[i] = item\n",
    "    label2id[item] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#메인 모델 토크나이저\n",
    "processor = DonutProcessor.from_pretrained(\"./pretrained/main/epoch-2, batch-2, img_size-480, max_length-560,copy/processor\")\n",
    "tokenizer = processor.tokenizer\n",
    "tokenizer_name='math_ocr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 키위파이 토크나이저, 메인 모델 토크나이저\n",
    "한글 정제 : 키위파이 토크나이저 <br>\n",
    "라텍스 정제 : 메인 모델 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def korean_tokenizer(texts):\n",
    "    \n",
    "    from kiwipiepy.utils import Stopwords\n",
    "    stopwords = Stopwords()\n",
    "    \n",
    "    add_words=['연립부등식','시속','유리함수','교점','밑변','부분합','쌓기나무','사인법칙','피타고라스','평행이동','대분수','도수분포표','이등변삼각형','상대도수','최솟값','중근','표본공간','닮음비',\n",
    "              '유한소수','연립부등식','공통범위','각뿔대','극한값','아랫변','수선의발','허근','중근','근과계수의관계','최단거리','교점','홀수항','짝수항','실근','부분집합',\n",
    "               '진리집합','공비','직사각형','예각삼각형','직각삼각형','일반항','부분집합','부등호','음의정수','항등함수','근의공식','수직이등분선','원주율','입체도형','준식','최댓값',\n",
    "               '증가량','감소량','일대일대응','대칭이동','음함수','좌변','우변','외심','조임정리','원주각','원순열','초항','점사이의거리','우함수','극댓값','내림차순','중심각','경우의수','기준량',\n",
    "               '대응각','수직이등분선','사잇값정리','접선의방정식','모눈칸','입체도형','표준형','변화량','벤다이어그램','최고차항','좌표평전','불연속','수직이등분선','도수분포표','이항','다항함수',\n",
    "               '원주율','연립부등식','정적분','소인수분해','일반항','최고항','상대도수','배반사건','증가함수','접선의기울기','삼수선','공통인수','무리식','일대일함수','닫힌구간','열린구간','기함수',\n",
    "               '진분수','호의길이','나머지정리','켤레복소수','무게중심','원순열','평균변화율','곱셈공식','이산확률','대응각','대등점','좌표평면','중복조합','차항계수']\n",
    "    \n",
    "    stop_words=['정답','개','학생','답','가능','의','라','가','을','은','과','장','철수','무승부','결과','축구','이란','열량','선택','권',\n",
    "                '동생','여유','테이프','곳','연주','한라산','현주','소금물','금요일','평가','하인','왼쪽','정확','사용','목걸이','여학생','영지',\n",
    "                '과자','종이','사진','출전','동안','성윤','탈','양궁','무','통장','자전거','개인','고등학교','되어','입장료','바깥','이때','감',\n",
    "                '시합','인정','도중','에','계절','이므로','옷','야구','를','밖','므','연필','오리','솟값','처음','동전','안','컵','텐트','만약',\n",
    "                '태준','역사책','어머니','사과','구슬','선생','만족','이용','식량','깃발','오렌지','화요일','반대','인','나','오른쪽','번','아치고',\n",
    "                '필요','여름','여야','거짓','장마','부분','은행','직','희연','톱니','주머니','서현','청소년','준','고','고르고','못','퀴','최고',\n",
    "                '풀','모','밀가루','후','메뚜기','월요일','하루','찰흙','정도','지현이','움','우유','기','서','떡','진아','샌드위치','모둠','시',\n",
    "                '승부','미생물','꽃','올해','단백질','색종이','입','작년','체인','머지','수박','매듭','자취','재경','달리기','팀','인형','초록색',\n",
    "                '무','이동','줄','제외','여학생','오','버스','추리','마찬가지','발사','세수','선수','조금','습관','회원','껌','영어','수면','하인',\n",
    "                '눈','사용','영어','임시','양말','바둑돌','각인','회원','잡지','독서량','대회','어린이','진아','르','남학생','여학생','아래','동쪽',\n",
    "                '어린이','상자','돈','학생','구슬','우유','서','해결','연습','발','맛','와','학교','공차','학기','조각','림','케이스','저금통','시행',\n",
    "                '하루','반','마을','위쪽','포도','설명','박테리아','부분','사탕','개수','회','도달','병','도','학교','아버지','영희','가지','아들',\n",
    "                '시작','방법','다음','대신','학업','키','과일','주스','포도','번호','각도기','시행','사탕','생각','후','다','의자','시계',\n",
    "                '모두','뒤','떡','연습','자리','톱니','어린이','다','학교','명','그릇','라','물','림','제비','학업','설명','다음','대신','몸무게',\n",
    "                '아래','은','이번','서','연습','배','작년','시작','동화책','생각','앞','제외','하루','학생','여학생','차','가지','방법','포도','와',\n",
    "                '형','우유','떡','진아','샌드위치','모둠','시','승부','그림','재범']\n",
    "\n",
    "    nums=['일','이','삼','사','오','육','칠','팔','구','십','십일','십이','이십']\n",
    "\n",
    "    for num in nums:\n",
    "        variant_words=['{}차방정식'.format(num),'정{}각형'.format(num),'{}각기둥'.format(num),'{}각형'.format(num),\n",
    "                       '{}차함수'.format(num),'{}차곡선'.format(num),'{}각뿔'.format(num),'{}차부등식'.format(num),\n",
    "                       '{}차식'.format(num)]\n",
    "        \n",
    "        for word in variant_words:\n",
    "            add_words.append(word)\n",
    "\n",
    "    for word in add_words:\n",
    "        kiwi.add_user_word(word,'NNG')\n",
    "\n",
    "    for word in stop_words:\n",
    "        stopwords.add((word, 'NNG'))\n",
    "\n",
    "    masks={'부분 집합':'부분집합','근과 계수와의 관계':'근과계수의관계','꼭짓점의 개수':'꼭짓점개수','다항 함수':'다항함수','접선의 방정식':'접선의방정식',\n",
    "           '닫힌 구간':'닫힌구간','열린 구간':'열린구간','이등변 삼각형':'이등변삼각형','나무쌓기':'쌓기나무','산술과 기하평균':'산술기하평균','산술 기하평균':'산술기하평균',\n",
    "           '각 기둥':'각기둥','차항의 계수':'차항계수','경우의 수':'경우의수','최대값':'최댓값','점 사이의 거리':'점사이의거리','사잇값 정리':'사잇값정리','사잇값의 정리':'사잇값정리','소인수 분해':'소인수분해',\n",
    "           '수선의 발':'수선의발','최단 거리':'최단거리','부분 집합':'부분집합','근의 공식':'근의공식','접선의 방정식':'접선의방정식','호의 길이':'호의길이','나머지 정리':'나머지 정리',\n",
    "           '접선의 기울기':'접선의기울기'}\n",
    "\n",
    "    # ground_truths 토크나이징 후 1차 정제 \n",
    "    uncleaned_ground_truths=[]\n",
    "    for text in texts:\n",
    "        print('원문','-',text)\n",
    "        text=text.replace('\\\\displaystyle','')\n",
    "        latexs=re.findall('<latex>.*?</latex>',text) # latex 부분만 추출\n",
    "        real_text=re.sub('<latex>.*?</latex>',\"\",text) # 텍스트 부분만 추출\n",
    "        \n",
    "        for mask in masks.keys():\n",
    "            \n",
    "            if mask in real_text: # text 부분에 mask dictonary 내의 key-->value 형태로 전환\n",
    "                \n",
    "                real_text=re.sub(mask,masks[mask],real_text)\n",
    "        \n",
    "        print('1차 가공','-',real_text)\n",
    "        \n",
    "        text_tokens=kiwi.tokenize(real_text,stopwords=stopwords)\n",
    "                \n",
    "        text_nn_tokens=[]\n",
    "        cleaned_latex_tokens=[]\n",
    "                \n",
    "        for t in text_tokens:\n",
    "            # 텍스트에서 명사만 추출\n",
    "            if t.tag=='NNG' or t.tag=='NNP':\n",
    "                text_nn_tokens.append(t.form)\n",
    "\n",
    "        if len(latexs)>=1: # 라텍스가 있는 부분에서만 작동\n",
    "            for latex in latexs:\n",
    "\n",
    "                latex_tokens=tokenizer.tokenize(latex)\n",
    "                latex_tokens=[x for x in latex_tokens if x.startswith('\\\\')] # \\\\sin,\\\\cos 등 필요한 기호만 필터링\n",
    "    \n",
    "\n",
    "                if len(latex_tokens)>=1:  \n",
    "                    cl_la_tk=' '.join(latex_tokens) # 라텍스를 한 문장으로 합침\n",
    "                    cleaned_latex_tokens.append(cl_la_tk)\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "        cleaned_text=' '.join(text_nn_tokens)\n",
    "        cleaned_latex=' '.join(cleaned_latex_tokens)\n",
    "        ct_cl=cleaned_text+cleaned_latex\n",
    "        uncleaned_ground_truths.append(ct_cl)\n",
    "        #print('가공 완료','-',ct_cl)\n",
    "        #print('----------------------------------------')\n",
    "\n",
    "    \n",
    "    return uncleaned_ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127060it [00:00, 2397985.36it/s]\n"
     ]
    }
   ],
   "source": [
    "def cleansing_texts_and_labels(texts):\n",
    "    \n",
    "    # 불용어 제거로 인한 sequence 길이 조절\n",
    "    \n",
    "    uncleaned_ground_truths=[]\n",
    "    uncleaned_labels=[]\n",
    "\n",
    "    tmp_gt_dicts={'e4':[],'e5':[],'e6':[],\n",
    "                  'm1':[],'m2':[],'m3':[],\n",
    "                  'h1':[],'h2':[],'h3':[]}\n",
    "\n",
    "    for i,v in tqdm(enumerate(texts)):\n",
    "        \n",
    "        # 해당 문장이 12자 이상이라면 그냥 패스\n",
    "        \n",
    "        if len(v)<=12:\n",
    "            tmp_gt_dicts[gt_labels[i]].append(v)\n",
    "            \n",
    "        # 해당 문장이 12자 미만이라면 임의의 해당 레이블의 딕셔너리로 매핑되어 리스트 형태로 들어감\n",
    "        \n",
    "        else:\n",
    "            uncleaned_ground_truths.append(v)\n",
    "            uncleaned_labels.append(gt_labels[i])\n",
    "\n",
    "    for k,v in tmp_gt_dicts.items():\n",
    "        \n",
    "        # 기본 3 단어씩 붙여서 하나의 sequence화\n",
    "        \n",
    "        while len(v)>=3:\n",
    "            \n",
    "            if len(v)>=3:\n",
    "                long_sent=r' '.join(v[:3])\n",
    "                v=v[3:]\n",
    "                uncleaned_ground_truths.append(long_sent)\n",
    "                uncleaned_labels.append(k)\n",
    "                \n",
    "            # 해당 딕셔너리에 단어가 3 단어 미만이라면 남은 단어들을 모두 join\n",
    "            \n",
    "            elif 1<=len(v)<=2:\n",
    "                long_sent=r' '.join(v)\n",
    "                v=[]\n",
    "                uncleaned_ground_truths.append(long_sent)\n",
    "                uncleaned_labels.append(k)\n",
    "\n",
    "    cleaned_ground_truths=[]\n",
    "    cleaned_labels=[]\n",
    "\n",
    "    for i,v in enumerate(tmp_ground_truths):\n",
    "        v=v.replace('\\\\','')\n",
    "        if len(v)>=2:\n",
    "            cleaned_ground_truths.append(v)\n",
    "            cleaned_labels.append(gt_labels[i])\n",
    "    \n",
    "    return cleaned_ground_truths,cleaned_labels\n",
    "\n",
    "cleaned_ground_truths,cleaned_labels=cleansing_texts_and_labels(uncleaned_ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_unk(texts):\n",
    "    ids = tokenizer(texts).input_ids\n",
    "    tokens = [tokenizer.tokenize(x) for x in texts]\n",
    "\n",
    "    unk_tokens = []\n",
    "    for example_ids, example_tokens in zip(ids, tokens):\n",
    "        example_unk_tokens = []\n",
    "        for i in range(len(example_ids)):\n",
    "            if example_ids[i] == tokenizer.unk_token_id:\n",
    "                try:\n",
    "                    example_unk_tokens.append(example_tokens[i])\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        unk_tokens += example_unk_tokens\n",
    "\n",
    "    return unk_tokens\n",
    "\n",
    "unk_tokens = check_for_unk(cleaned_ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "DFr3Q5UZrpXT",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "text_unk_tokens = list(set(unk_tokens))\n",
    "print(len(text_unk_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow_hub as hub \n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(x):\n",
    "    numid=label2id[x]\n",
    "    return numid\n",
    "\n",
    "num_split=int(len(cleaned_labels)*0.8)\n",
    "\n",
    "y_encoding = list(map(encoding,cleaned_labels))\n",
    "y_one_hot=tf.one_hot(y_encoding, depth=9)\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    y_one_hot_np = sess.run(y_one_hot)\n",
    "\n",
    "# 데이터를 훈련 데이터와 테스트 데이터로 분할\n",
    "num_split = int(len(cleaned_labels) * 0.8)\n",
    "X_train = np.asarray(cleaned_ground_truths[:num_split])\n",
    "y_train = np.asarray(y_one_hot_np[:num_split])\n",
    "X_test = np.asarray(cleaned_ground_truths[num_split:])\n",
    "y_test = np.asarray(y_one_hot_np[num_split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62240, 9)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62240,)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_one_hot = tf.one_hot(labels, depth=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "variable_scope module_2/ was unused but the corresponding name_scope was already taken.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m elmo \u001b[38;5;241m=\u001b[39m \u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://tfhub.dev/google/elmo/2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/team_conda/lib/python3.10/site-packages/tensorflow_hub/module.py:165\u001b[0m, in \u001b[0;36mModule.__init__\u001b[0;34m(self, spec, trainable, name, tags)\u001b[0m\n\u001b[1;32m    162\u001b[0m   tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(tags)) \u001b[38;5;28;01mif\u001b[39;00m tags \u001b[38;5;28;01melse\u001b[39;00m tags\n\u001b[1;32m    163\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such graph variant: tags=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m tags)\n\u001b[0;32m--> 165\u001b[0m abs_state_scope \u001b[38;5;241m=\u001b[39m \u001b[43m_try_get_state_scope\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmark_name_scope_used\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m abs_state_scope\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    168\u001b[0m abs_parent_scope \u001b[38;5;241m=\u001b[39m abs_state_scope\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/team_conda/lib/python3.10/site-packages/tensorflow_hub/module.py:401\u001b[0m, in \u001b[0;36m_try_get_state_scope\u001b[0;34m(name, mark_name_scope_used)\u001b[0m\n\u001b[1;32m    399\u001b[0m   unique_name_scope \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39munique_name(name, mark_name_scope_used) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    400\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m unique_name_scope \u001b[38;5;241m!=\u001b[39m abs_state_scope:\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable_scope \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m was unused but the corresponding \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_scope was already taken.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m abs_state_scope)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m abs_state_scope\n",
      "\u001b[0;31mRuntimeError\u001b[0m: variable_scope module_2/ was unused but the corresponding name_scope was already taken."
     ]
    }
   ],
   "source": [
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cachetools import TTLCache,cached\n",
    "cache = TTLCache(maxsize=100, ttl=21600)\n",
    "\n",
    "@cached(cache)\n",
    "def ELMoEmbedding(x):\n",
    "    return elmo(tf.reshape(tf.cast(x,tf.string),[-1]),signature='default',as_dict=True)['elmo']\n",
    "\n",
    "def create_model_architecture():\n",
    "    input_text = tf.keras.layers.Input(shape=(1,), dtype='string',name='input_text')\n",
    "    embedding = tf.keras.layers.Lambda(ELMoEmbedding,output_shape = (1024,), name = 'elmo_embedding')(input_text)\n",
    "\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1024,return_sequences=False,recurrent_dropout=0.2,dropout=0.2,name=\"BiLSTM\"))(embedding)\n",
    "    x = tf.keras.layers.Dense(512,activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    #x = embedding\n",
    "    x = tf.keras.layers.Dense(256,activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    predictions = tf.keras.layers.Dense(9,activation='softmax')(x)\n",
    "\n",
    "\n",
    "    model = tf.keras.Model(inputs = [input_text],outputs = predictions)\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['categorical_accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_text (InputLayer)     [(None, 1)]               0         \n",
      "                                                                 \n",
      " elmo_embedding (Lambda)     (None, None, 1024)        0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 2048)              16785408  \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               1049088   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 9)                 2313      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17968137 (68.54 MB)\n",
      "Trainable params: 17968137 (68.54 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model_architecture()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49792 samples, validate on 12448 samples\n",
      "Epoch 1/3\n",
      "49792/49792 [==============================] - ETA: 0s - loss: 1.6891 - acc: 0.3647"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-06 15:05:19.059037: W tensorflow/c/c_api.cc:304] Operation '{name:'loss_5/mul' id:16856 op device:{requested: '', assigned: ''} def:{{{node loss_5/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_5/mul/x, loss_5/dense_11_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49792/49792 [==============================] - 676s 14ms/sample - loss: 1.6891 - acc: 0.3647 - val_loss: 1.5064 - val_acc: 0.4296\n",
      "Epoch 2/3\n",
      "49792/49792 [==============================] - 671s 13ms/sample - loss: 1.4977 - acc: 0.4332 - val_loss: 1.4288 - val_acc: 0.4645\n",
      "Epoch 3/3\n",
      "49792/49792 [==============================] - 678s 14ms/sample - loss: 1.4247 - acc: 0.4645 - val_loss: 1.3833 - val_acc: 0.4860\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    sess.run(tf.compat.v1.tables_initializer())\n",
    "    # history = model.fit(np.asarray(df_train.clean_tweet),y_train,epochs=5,batch_size = 2,validation_split=0.2)\n",
    "    history = model.fit(X_train,y_train,epochs=3,batch_size = 32,validation_split=0.2)\n",
    "    model.save_weights('./pretrained/Sub/elmo-model')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "team_conda",
   "language": "python",
   "name": "team_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
